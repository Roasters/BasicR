install.packages(c("ROAuth", "base64enc"))
library("twitteR")
library("ROAuth")
library("base64enc")
a0dAmPix5K0dHN6b"
consumerSecret <- "h2X6D6bNUzEwayZE5KTDxDI9HNou4hzDW69akAQPHjCPctxmFU"
accessToken <- "1024082298-PlMdoQEPFyo77AXQTPxtkFKbaGuqnbBO4L2ILKY"
accessTokenSecret <- "H3GOSsRVrIaaPHwjThPCftN0cuL8xUF53sAc2oubbz7Sn"
# oauth 인증 파일 저장
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
# 콘솔 창에 1(yes) 선택
#키워드 저장
keyword <-enc2utf8("기생충")
# 크롤링할 트위터 수(n=1000)와 언어(lang="ko")
data<- searchTwitter(keyword, n=1000, lang="ko")
length(data)
consumerKey <- "zxMMmkzf7a0dAmPix5K0dHN6b"
consumerSecret <- "h2X6D6bNUzEwayZE5KTDxDI9HNou4hzDW69akAQPHjCPctxmFU"
accessToken <- "1024082298-PlMdoQEPFyo77AXQTPxtkFKbaGuqnbBO4L2ILKY"
accessTokenSecret <- "H3GOSsRVrIaaPHwjThPCftN0cuL8xUF53sAc2oubbz7Sn"
# oauth 인증 파일 저장
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
#키워드 저장
keyword <-enc2utf8("기생충")
# 크롤링할 트위터 수(n=1000)와 언어(lang="ko")
data<- searchTwitter(keyword, n=1000, lang="ko")
length(data)
data
library(rvest)
library(stringr)
url_num = c(32474:42474)
url_default = "https://www.koreapas.com/bbs/view.php?id=kfc&page=1&sn1=&divpage=8&sn=off&ss=on&sc=on&select_arrange=headnum&desc=asc&no="
library(KoNLP)
useSejongDic()
?useSejongDic()
useSejongDic
?html_noe
?html_node
사람 %in% useSejongDic()
'사람' %in% useSejongDic()
?KoNLP
url = "https://www.koreapas.com/bbs/view.php?id=kfc&page=1&sn1=&divpage=8&sn=off&ss=on&sc=on&select_arrange=headnum&desc=asc&no=42474"
htxt = read_html(url)
htxt = read_html(url, encoding="cp949")
htxt = read_html(url, encoding="euc-kr")
htxt = read_html(url, encoding="ko")
htxt = read_html(url, encoding = 'utf-8')
htxt = read_html(url, encoding = 'utf8')
htxt = read_html(url, encoding = 'cp949')
?encoding
guess_encoding(url)
htxt = read_html(url, encoding = 'ISO-8859-1')
html_node(htxt, ".hansb")
html_nodes(htxt, ".hansb")
htxt = read_html(url, encoding = 'ISO-8859-2')
html_nodes(htxt, ".hansb")
html_text(html_nodes(htxt, ".hansb"))
url = "https://www.koreapas.com/bbs/view.php?id=kfc&page=1&sn1=&divpage=8&sn=off&ss=on&sc=on&select_arrange=headnum&desc=asc&no=42473"
htxt = read_html(url, encoding = 'ISO-8859-1')
html_text(html_nodes(htxt, ".hansb"))
url = "https://www.koreapas.com/bbs/view.php?id=kfc&page=1&sn1=&divpage=8&sn=off&ss=on&sc=on&select_arrange=headnum&desc=asc&no=42473"
htxt = read_html(url, encoding = 'ISO-8859-2')
html_text(html_nodes(htxt, ".hansb"))
url = "https://www.koreapas.com/bbs/view.php?id=kfc&page=1&sn1=&divpage=8&sn=off&ss=on&sc=on&select_arrange=headnum&desc=asc&no=42473"
htxt = read_html(url, encoding = 'cp949')
html_text(html_nodes(htxt, ".hansb"))
guess_encoding(url)
htxt = read_html(url, encoding = 'UTF-8')
htxt = read_html(url, encoding = 'EUC-KR')
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
html_text(html_nodes(htxt, ".rz6fp9-10"))
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
html_text(html_nodes(htxt, ".rz6fp9-10"))
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
html_text(html_nodes(htxt, "href"))
html_text(html_nodes(htxt, "#href"))
html_attr(htxt, "href")
htxt %>% html_text(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr(htxt, "href")
htxt %>% html_text(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
htxt %>% html_attr("href")
htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attrs("href")
htxt %>% html_attrs("href")
html_attrs(htxt, "href")
html_attrs(htxt)
html_attrs(html_nodes(htxt))
html_attrs(html_nodes(htxt, ".SQnoC3ObvgnGjWt90zD9Z"))
html_attrs(html_nodes(htxt, ".SQnoC3ObvgnGjWt90zD9Z"))["href"]
htxt %>% html_nodes("SQnoC3ObvgnGjWt90zD9Z") %>% html_attrs("href")
htxt %>% html_nodes("SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
htxt %>% html_nodes("SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
htxt %>% html_node("SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
htxt %>% html_nodes("SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
url = "https://www.reddit.com/r/MachineLearning/"
htxt = read_html(url)
htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
links = htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
class(links)
url_default = "https://www.reddit.com/r/MachineLearning/"
links
url_default = "https://www.reddit.com/"
url_default = "https://www.reddit.com"
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
comment = html_nodes(htmlTxt, ".rz6fp9")
allComments = c(allComments, html_text(comment))
}
allComments
links
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
print(htmlTxt)
comment = html_nodes(htmlTxt, ".rz6fp9")
allComments = c(allComments, html_text(comment))
}
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
print(htmlTxt)
comment = html_nodes(htmlTxt, ".rz6fp9")
print(comment)
allComments = c(allComments, html_text(comment))
}
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
print(htmlTxt)
comment = html_nodes(htmlTxt, ".rz6fp9.himkey")
print(comment)
allComments = c(allComments, html_text(comment))
}
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
print(htmlTxt)
comment = html_nodes(htmlTxt, ".rz6fp9 himkey")
print(comment)
allComments = c(allComments, html_text(comment))
}
url = paste(url_default, links[[1]], sep="")
url
htmlTxt = read_html(url)
htmlTxt
html_nodes(htmlTxt, ".rz6fp9-10 himKiy")
html_nodes(htmlTxt, ".rz6fp9-10")
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
comment = html_nodes(htmlTxt, ".rz6fp9-10")
allComments = c(allComments, html_text(comment))
}
allComments
htxt = read_html("https://www.reddit.com/r/MachineLearning/hot/")
links = htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
links
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
comment = html_nodes(htmlTxt, ".rz6fp9-10")
allComments = c(allComments, html_text(comment))
}
allComments
links
str_replace_all(allComments, "[.*]", "")
str_replace_all(allComments, "[.*]|[[::punct]]+", "")
str_replace_all(allComments, "[.*]|[^\\w^\\s]+", "")
text <- str_replace_all(allComments, "[.*]|[^\\w^\\s]+", "")
text <- str_replace_all(allComments, "[.*]|[^\\w^\\s]+|\\(|\\)", "")
text <- str_replace_all(allComments, "[.*]|[^\\w^\\s]+|", "")
text = str_replace_all(allComments, "[.*]|[^\\w^\\s]+|", "")
text = tolower(text)
text
text = str_split(text)
text = str_split(text, " ")
text
table(unlist(text))
library(NLP)
library(tm)
text = str_replace_all(allComments, "[.*]|[^\\w^\\s]+|", "")
text = tolower(text)
words = unlist(str_split(text, " "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
words_meaningful
table(words_meaningful)
length(table(words_meaningful))
table(sort(words_meaningful))
head(table(sort(words_meaningful)))
text = str_replace_all(allComments, "[.*]|[^a-zA-z^\\s]+|", "")
text = tolower(text)
words = unlist(str_split(text, " "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
head(table(sort(words_meaningful)))
text = str_replace_all(allComments, "[.+]|[^a-zA-z^\\s]+|", "")
text = tolower(text)
words = unlist(str_split(text, " "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
head(table(sort(words_meaningful)))
text = str_replace_all(allComments, "\\[.+\\]|[^a-zA-z^\\s]+|", "")
text = tolower(text)
words = unlist(str_split(text, " "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
head(table(sort(words_meaningful)))
stopwords("SMART")
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.+\\]|[[::punct::]]|", "")
text = tolower(text)
words = unlist(str_split(text, " "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[:punct:]", "")
text = tolower(text)
Noun = extractNoun(text)
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]", "")
text = tolower(text)
Noun = extractNoun(text)
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]", "")
text = tolower(text)
words = str_split(text, pattern=" ")
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]", "")
text = tolower(text)
words = str_split(text, pattern=" ")
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
table(words_meaningful)
words_meaningful
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]", "")
text = tolower(text)
words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
table(words_meaningful)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]+|\\d+", "")
text = tolower(text)
words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]+|\\d+|~|\\^|\\$", "")
text = tolower(text)
words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]+|\\d+|~|\\^|\\$|\\||=", "")
text = tolower(text)
words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]+|\\d+|~|\\^|\\$|\\||=|<.*>", "")
text = tolower(text)
words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
text = str_replace_all(allComments, "\\[.*\\]|[[:punct:]]+|\\d+|~|\\^|\\$|\\||=|\\<.*\\>", "")
text = tolower(text)
words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(words %in% stopwords(kind="SMART"), NA, words)
data.frame(table(words_meaningful))
words_df = data.frame(table(words_meaningful))
words_df %>%
arrange(desc(freq))
library(dplyr)
words_df %>%
arrange(desc(freq))
words_df %>%
arrange(desc(Freq))
words_df %>%
arrange(desc(Freq)) %>%
head(30)
words_df %>%
arrange(desc(Freq)) %>%
head(50)
words_df = words_df[words_df$Freq >= 4]
words_df$Freq >= 4
words_df = words_df[words_df$Freq >= 4, ]
words_df
sort(words_df)
sort(words_df$Freq)
words_df %>%
arrange(desc(Freq))
install.packages("tidytext")
library(tidytext)
get_sentiments("nrc")
summary(get_sentiments("afinn"))
AFINN<-data.frame(get_sentiments("afinn"))
hist(AFINN$score, xlim=c(-6,6),col='blue', breaks=20)
get_sentiments("bing")
oplex<-data.frame(get_sentiments("bing"))
table(oplex$sentiment)
emolex<-data.frame(get_sentiments("nrc"))
table(emolex$sentiment)
emolex$word[emolex$sentiment=="anger"]
library(tm)
library(stringr)
library(dplyr)
my.text.location<-"papers/"
my.text.location<-"C:/rwork/papers/"
mypaper<-VCorpus(DirSource(my.text.location))
inspect(mypaper)
length(as.character(mypaper[[1]][1]))#1
length(unlist(mypaper[[1]][1]))#1
my.df.text<-data_frame(paper.id=1:24, doc=mytxt)
mytxt<-c(rep(NA,24))
mytxt
for(i in 1:24){
mytxt[i]<-as.character(mypaper[[i]][1])
}
my.df.text<-data_frame(paper.id=1:24, doc=mytxt)
my.df.text
my.df.text.word<-my.df.text %>%
unnest_tokens(word, doc)
#unnest_tokens:문서 단위의 텍스트를 단어로 분해
library(tidyr)
myresult.sa<-my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word, paper.id, sentiment) %>%
spread(sentiment,n,fill=0)
myresult.sa
myagg<-summarise(group_by(myresult.sa, paper.id),
pos.sum=sum(positive),
neg.sum=sum(negative),
pos.sent=pos.sum-neg.sum)
myagg
myfilenames<-list.files(path=my.text.location,
all.files = TRUE)
paper.name<-myfilenames[3:26]
#pub.year<-as.numeric(unlist(str_extract_all(paper.name, "[[:digit:]]{4}")))
pub.year<-as.numeric(str_extract_all(paper.name, "[[:digit:]]{4}"))
pub.year
paper.id<-1:24
pub.year.df<-data.frame(paper.id, paper.name, pub.year)
pub.year.df
unnest_tokens(text)
?unnest_tokens
library(rvest)
library(stringr)
library(tm)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
url_default = "https://www.reddit.com"
htxt = read_html("https://www.reddit.com/r/MachineLearning/hot/")
links = htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
links
allComments = list()
for (i in length(links)) {
url = paste(url_default, links[i], sep="")
htmlTxt = read_html(url)
comment = html_nodes(htmlTxt, ".rz6fp9-10")
allComments[[i]] = html_text(comment)
}
allComments
links[1]
links[2]
allComments = list()
for (i in 1:length(links)) {
url = paste(url_default, links[i], sep="")
htmlTxt = read_html(url)
comment = html_nodes(htmlTxt, ".rz6fp9-10")
allComments[[i]] = html_text(comment)
}
allComments
mytxt<-c(rep(NA,length(links)))
library(rvest)
library(stringr)
library(tm)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
url_default = "https://www.reddit.com"
htxt = read_html("https://www.reddit.com/r/MachineLearning/hot/")
links = htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
comment = html_nodes(htmlTxt, ".rz6fp9-10")
allComments = c(allComments, html_text(comment))
}
my.df.text<-data_frame(comment.id=1:length(allComments), doc=allComments)
my.df.text
my.df.text.word<-my.df.text %>%
unnest_tokens(word, doc)
my.df.text.word
library(tidyr)
table(my.df.text.word)
temp = tolower(my.df.text.word)
# words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(temp %in% stopwords(kind="SMART"), NA, temp)
my_words = str_replace_all(words_meaningful, "[^a-z]+", "")
my_words = ifelse(my_words == "", NA, my_words)
my_words
temp = tolower(my.df.text.word)
temp
my.df.text.word
temp = tolower(my.df.text.word$word)
# words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(temp %in% stopwords(kind="SMART"), NA, temp)
my_words = str_replace_all(words_meaningful, "[^a-z]+", "")
my_words = ifelse(my_words == "", NA, my_words)
my_words
table(my_words)
my.df.text.word$word = my_words
my.df.text.word
words_df = data.frame(table(my_words))
words_df = words_df[words_df$Freq >= 4, ]
cmap = brewer.pal(8, "Pastel1")
wordcloud(words_df$my_words, freq=words_df$Freq, min.freq = 4, col = cmap, random.order = FALSE, scale=c(4,0.2))
my.df.text.word$word = my_words
wer(my.df.text.word$word)
# words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(temp %in% stopwords(kind="SMART"), NA, temp)
my_words = str_replace_all(words_meaningful, "[^a-z]+", "")
my_words = ifelse(my_words == "", NA, my_words)
my.df.text.word$word = my_words
table_df = data.frame(table(my.df.text.word$word))
table_df = table_df[words_df$Freq >= 4, ]
cmap = brewer.pal(8, "Pastel1")
wordcloud(tab
temp = tolower(my.df.text.word$word)
# words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(temp %in% stopwords(kind="SMART"), NA, temp)
my_words = str_replace_all(words_meaningful, "[^a-z]+", "")
my_words = ifelse(my_words == "", NA, my_words)
my.df.text.word$word = my_words
table_df = data.frame(table(my.df.text.word$word))
table_df = table_df[words_df$Freq >= 4, ]
cmap = brewer.pal(8, "Pastel1")
wordcloud(table_df$my_words, freq=table_df$Freq, min.freq = 4, col = cmap, random.order = FALSE, scale=c(4,0.2))
my.df.text<-data_frame(comment.id=1:length(allComments), doc=allComments)
my.df.text.word<-my.df.text %>%
unnest_tokens(word, doc)
temp = tolower(my.df.text.word$word)
# words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(temp %in% stopwords(kind="SMART"), NA, temp)
my_words = str_replace_all(words_meaningful, "[^a-z]+", "")
my_words = ifelse(my_words == "", NA, my_words)
my.df.text.word$word = my_words
table_df = data.frame(table(my.df.text.word$word))
table_df = table_df[words_df$Freq >= 4, ]
cmap = brewer.pal(8, "Pastel1")
wordcloud(table_df$my_words, freq=table_df$Freq, min.freq = 4, col = cmap, random.order = FALSE, scale=c(4,0.2))
library(rvest)
library(stringr)
library(tm)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
library(tidytext)
library(tidyr)
url_default = "https://www.reddit.com"
htxt = read_html("https://www.reddit.com/r/MachineLearning/hot/")
links = htxt %>% html_nodes(".SQnoC3ObvgnGjWt90zD9Z") %>% html_attr("href")
allComments = c()
for (link in links) {
url = paste(url_default, link, sep="")
htmlTxt = read_html(url)
comment = html_nodes(htmlTxt, ".rz6fp9-10")
allComments = c(allComments, html_text(comment))
}
my.df.text<-data_frame(comment.id=1:length(allComments), doc=allComments)
my.df.text.word<-my.df.text %>%
unnest_tokens(word, doc)
temp = tolower(my.df.text.word$word)
# words = unlist(str_split(text, pattern=" "))
words_meaningful = ifelse(temp %in% stopwords(kind="SMART"), NA, temp)
my_words = str_replace_all(words_meaningful, "[^a-z]+", "")
my_words = ifelse(my_words == "", NA, my_words)
my.df.text.word$word = my_words
table_df = data.frame(table(my.df.text.word$word))
table_df = table_df[words_df$Freq >= 4, ]
cmap = brewer.pal(8, "Pastel1")
wordcloud(table_df$my_words, freq=table_df$Freq, min.freq = 4, col = cmap, random.order = FALSE, scale=c(4,0.2))
table_df
head(table_df)
wordcloud(table_df$Var1, freq=table_df$Freq, min.freq = 4, col = cmap, random.order = FALSE, scale=c(4,0.2))
myresult.sa<-my.df.text.word %>%
inner_join(get_sentiments("bing")) %>%
count(word, comment.id, sentiment) %>%
spread(sentiment,n,fill=0)
myresult.sa
myagg<-summarise(group_by(myresult.sa, comment.id),
pos.sum=sum(positive),
neg.sum=sum(negative),
pos.sent=pos.sum-neg.sum)
myagg
